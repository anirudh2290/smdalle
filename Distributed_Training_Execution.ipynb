{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Multigpu Distributed Training-ScriptMode\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API을 효과적으로 이용하기 위해 multigpu-distributed 학습을 위한 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "#     !{sys.executable} -m pip install -U split-folders tqdm albumentations crc32c wget\n",
    "    !{sys.executable} -m pip install 'sagemaker[local]' --upgrade\n",
    "    !{sys.executable} -m pip install -U bokeh smdebug sagemaker-experiments\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    !/bin/bash ./local/local_mode_setup.sh\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정\n",
    "\n",
    "<p>Sagemaker 학습에 필요한 기본적인 package를 import 합니다. </p>\n",
    "<p>boto3는 HTTP API 호출을 숨기는 편한 추상화 모델을 가지고 있고, Amazon EC2 인스턴스 및 S3 버켓과 같은 AWS 리소스와 동작하는 파이선 클래스를 제공합니다. </p>\n",
    "<p>sagemaker python sdk는 Amazon SageMaker에서 기계 학습 모델을 교육 및 배포하기 위한 오픈 소스 라이브러리입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "# import splitfolders\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "# import wget\n",
    "# import tarfile\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from time import strftime\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "from sagemaker.debugger import (Rule,\n",
    "                                rule_configs,\n",
    "                                ProfilerConfig, \n",
    "                                FrameworkProfile, \n",
    "                                DetailedProfilingConfig, \n",
    "                                DataloaderProfilingConfig, \n",
    "                                PythonProfilingConfig)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.35.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name):\n",
    "    try:\n",
    "        sm_experiment = Experiment.load(experiment_name)\n",
    "    except:\n",
    "        sm_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                          tags=[\n",
    "                                              {\n",
    "                                                  'Key': 'multigpu',\n",
    "                                                  'Value': 'yes'\n",
    "                                              },\n",
    "                                              {\n",
    "                                                  'Key': 'multinode',\n",
    "                                                  'Value': 'yes'\n",
    "                                              },\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial(experiment_name, set_param, i_type, i_cnt, spot):\n",
    "    create_date = strftime(\"%m%d-%H%M\")\n",
    "    \n",
    "    spot = 's' if spot else 'd'\n",
    "    i_tag = 'test'\n",
    "    if i_type == 'ml.p3.16xlarge':\n",
    "        i_tag = 'p3'\n",
    "    elif i_type == 'ml.p3dn.24xlarge':\n",
    "        i_tag = 'p3dn'\n",
    "    elif i_type == 'ml.p4d.24xlarge':\n",
    "        i_tag = 'p4d'    \n",
    "        \n",
    "    trial = \"-\".join([i_tag,str(i_cnt),spot])\n",
    "       \n",
    "    sm_trial = Trial.create(trial_name=f'{experiment_name}-{trial}-{create_date}',\n",
    "                            experiment_name=experiment_name)\n",
    "\n",
    "    job_name = f'{sm_trial.trial_name}'\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'bucket-exp-dalle-210410'\n",
    "code_location = f's3://{bucket}/sm_codes'\n",
    "output_path = f's3://{bucket}/poc_dalle/output/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'train:lr', 'Regex': 'lr - (.*?),'},\n",
    "     {'Name': 'train:Loss', 'Regex': 'loss -(.*?),'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "\n",
    "rules=[ \n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'EPOCHS' : 1,\n",
    "        'BATCH_SIZE' : 32, # 24\n",
    "        'LEARNING_RATE' : 1e-3,\n",
    "        'LR_DECAY_RATE' : 0.98,\n",
    "        'NUM_TOKENS' : 8192,\n",
    "        'NUM_LAYERS' : 2,\n",
    "        'NUM_RESNET_BLOCKS' : 2,\n",
    "        'SMOOTH_L1_LOSS' : False,\n",
    "        'EMB_DIM' : 512,\n",
    "        'HID_DIM' : 256,\n",
    "        'KL_LOSS_WEIGHT' : 0,\n",
    "        'STARTING_TEMP' : 1.,\n",
    "        'TEMP_MIN' : 0.5,\n",
    "        'ANNEAL_RATE' : 1e-6,\n",
    "        'NUM_IMAGES_SAVE' : 4,\n",
    "        'model_parallel': True,  ## False : DeepSpeeds\n",
    "        'num_microbatches': 8,\n",
    "        'num-partitions' : 2,\n",
    "        'placement_strategy': 'spread',\n",
    "        'pipeline': 'interleaved',\n",
    "        'optimize': 'speed',\n",
    "        'ddp': True,\n",
    "    }\n",
    "\n",
    "experiment_name = 'dalle-poc-exp1'\n",
    "instance_type = 'local_gpu'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "instance_count = 1\n",
    "do_spot_training = False\n",
    "max_wait = None\n",
    "max_run = 2*60*60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "    from pathlib import Path\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    s3_data_path = 'file:///home/ec2-user/SageMaker/napkin-Dalle/dataset'\n",
    "    source_dir = f'{Path.cwd()}/source_code'\n",
    "else:\n",
    "    sess = boto3.Session()\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sm = sess.client('sagemaker')\n",
    "    bucket_name = 'dataset-cyj-coco-210410'\n",
    "    s3_data_path = f's3://{bucket_name}/dataset'\n",
    "    source_dir = 'source_code'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_job_name : smp-dist \n",
      "train_instance_type : local_gpu \n",
      "train_instance_count : 1 \n",
      "image_uri : None \n",
      "distribution : {'smdistributed': {'modelparallel': {'enabled': True, 'parameters': {'partitions': 2}}}, 'mpi': {'enabled': True, 'processes_per_host': 2, 'custom_mpi_options': '-verbose -x orte_base_help_aggregate=0 '}}\n"
     ]
    }
   ],
   "source": [
    "image_uri = None\n",
    "distribution = None\n",
    "train_job_name = 'sagemaker'\n",
    "\n",
    "\n",
    "train_job_name = 'smp-dist'\n",
    "\n",
    "distribution = {\"smdistributed\": {\n",
    "                  \"modelparallel\": {\n",
    "                      \"enabled\":True,\n",
    "                      \"parameters\": {\n",
    "                          \"partitions\": hyperparameters['num-partitions'],\n",
    "#                               \"microbatches\": 8,\n",
    "#                               \"placement_strategy\": \"spread\",\n",
    "#                               \"pipeline\": \"interleaved\",\n",
    "#                               \"optimize\": \"speed\",\n",
    "#                               \"partitions\": 1,\n",
    "#                               \"ddp\": True,\n",
    "                      }\n",
    "                  }\n",
    "              },\n",
    "              \"mpi\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"processes_per_host\": 2, # Pick your processes_per_host\n",
    "                    \"custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0 \"\n",
    "              },\n",
    "          }\n",
    "\n",
    "\n",
    "if do_spot_training:\n",
    "    max_wait = max_run\n",
    "\n",
    "print(\"train_job_name : {} \\ntrain_instance_type : {} \\ntrain_instance_count : {} \\nimage_uri : {} \\ndistribution : {}\".format(train_job_name, instance_type, instance_count, image_uri, distribution))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 µs, sys: 52 µs, total: 185 µs\n",
      "Wall time: 190 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='train_vae.py',\n",
    "    source_dir=source_dir,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "#     volume_size=1024,\n",
    "    code_location = code_location,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution=distribution,\n",
    "#     disable_profiler=True,\n",
    "    metric_definitions=metric_definitions,\n",
    "    rules=rules,\n",
    "    max_run=max_run,\n",
    "    use_spot_instances=do_spot_training,  # spot instance 활용\n",
    "    max_wait=max_wait,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: dalle-poc-exp1-test-1-d-0417-0639\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-1svdy:\n",
      "    command: train\n",
      "    container_name: 5y6zfj3v6k-algo-1-1svdy\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.7.1-gpu-py36\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-1svdy\n",
      "    runtime: nvidia\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmp33k2xzuj/algo-1-1svdy/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmp33k2xzuj/algo-1-1svdy/output:/opt/ml/output\n",
      "    - /tmp/tmp33k2xzuj/algo-1-1svdy/input:/opt/ml/input\n",
      "    - /tmp/tmp33k2xzuj/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "    - /home/ec2-user/SageMaker/napkin-Dalle/dataset:/opt/ml/input/data/training\n",
      "    - /home/ec2-user/SageMaker/napkin-Dalle/smdalle/source_code:/opt/ml/code\n",
      "    - /tmp/tmp33k2xzuj/shared:/opt/ml/shared\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmp33k2xzuj/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 5y6zfj3v6k-algo-1-1svdy ... \n",
      "Creating 5y6zfj3v6k-algo-1-1svdy ... done\n",
      "Attaching to 5y6zfj3v6k-algo-1-1svdy\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:39:52,826 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:39:52,907 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:39:52,910 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:39:52,911 sagemaker-training-toolkit INFO     Installing module with the following command:\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m /opt/conda/bin/python3.6 -m pip install . -r requirements.txt\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting wandb\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading wandb-0.10.26-py2.py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 15.7 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting axial-positional-embedding\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting einops\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.51.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting omegaconf\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting pytorch-lightning\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading pytorch_lightning-1.2.8-py3-none-any.whl (841 kB)\n",
      "\u001b[K     |████████████████████████████████| 841 kB 53.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting deepspeed\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading deepspeed-0.3.14.tar.gz (341 kB)\n",
      "\u001b[K     |████████████████████████████████| 341 kB 59.1 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting albumentations\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading albumentations-0.5.2-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: torchnet in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (0.0.4)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting DALL-E\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading DALL_E-0.1-py3-none-any.whl (6.0 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting ftfy\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading ftfy-6.0.1.tar.gz (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.5 MB/s  eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from dalle-pytorch==0.9.5) (8.2.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting regex\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 50.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting taming-transformers\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.6/site-packages (from dalle-pytorch==0.9.5) (1.7.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from dalle-pytorch==0.9.5) (0.8.2)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->dalle-pytorch==0.9.5) (3.7.4.3)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->dalle-pytorch==0.9.5) (0.8)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->dalle-pytorch==0.9.5) (1.19.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting imgaug>=0.4.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 53.5 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 8)) (1.5.4)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting scikit-image>=0.16.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 47.8 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 8)) (5.4.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting opencv-python-headless>=4.1.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.6 MB 54.5 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting imageio\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 51.9 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 8)) (1.15.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting opencv-python\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.4 MB 48.0 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting Shapely\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 65.4 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 8)) (3.3.4)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting tifffile>=2019.7.26\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 69.9 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 65.5 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 8)) (2.5.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 8)) (2.4.7)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 8)) (2.8.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 8)) (1.3.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 8)) (0.10.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 8)) (4.4.2)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting tensorboardX==1.8\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 73.6 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting ninja\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 71.4 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from deepspeed->-r requirements.txt (line 7)) (5.8.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.8->deepspeed->-r requirements.txt (line 7)) (3.15.7)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: fsspec[http]>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.9.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.18.2)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting tensorboard>=2.2.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 48.5 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting torchmetrics>=0.2.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
      "\u001b[K     |████████████████████████████████| 176 kB 73.4 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting PyYAML\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 77.1 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (3.10.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (2.25.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting aiohttp\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading aiohttp-3.7.4.post0-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 56.1 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading google_auth-1.29.0-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 73.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 6)) (0.35.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 6)) (1.0.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting markdown>=2.6.8\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting grpcio>=1.24.3\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading grpcio-1.37.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 66.2 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting absl-py>=0.4\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 72.0 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 6)) (49.6.0.post20210108)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting tensorboard-plugin-wit>=1.6.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 42.9 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 6)) (4.7.2)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 70.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting requests-oauthlib>=0.7.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 6)) (0.4.8)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (3.0.4)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (2020.12.5)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (2.10)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (1.25.11)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting oauthlib>=3.0.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 71.7 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: visdom in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 9)) (0.1.8.9)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting GitPython>=1.0.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 76.8 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting promise<3,>=2.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading promise-2.3.tar.gz (19 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting sentry-sdk>=0.4.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading sentry_sdk-1.0.0-py2.py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 69.0 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting subprocess32>=3.5.3\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting pathtools\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting shortuuid>=0.5.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting configparser>=3.8.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (7.1.2)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting docker-pycreds>=0.4.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting gitdb<5,>=4.0.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting smmap<5,>=3.0.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting async-timeout<4.0,>=3.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 75.8 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting idna-ssl>=1.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 71.1 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (20.3.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting mypy\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading mypy-0.812-cp36-cp36m-manylinux2010_x86_64.whl (21.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 25.8 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting pytest\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 74.0 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting blobfile\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading blobfile-0.11.0-py3-none-any.whl (32 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting xmltodict~=0.12.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting pycryptodomex~=3.8\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading pycryptodomex-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 53.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting filelock~=3.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->dalle-pytorch==0.9.5) (0.2.5)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->fsspec[http]>=0.8.1->pytorch-lightning->-r requirements.txt (line 6)) (3.4.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting mypy-extensions<0.5.0,>=0.4.3\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting typed-ast<1.5.0,>=1.4.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading typed_ast-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 41.3 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting py>=1.8.2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25hCollecting toml\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting pluggy<1.0.0a1,>=0.12\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Collecting iniconfig\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from pytest->DALL-E->dalle-pytorch==0.9.5) (20.9)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: pyzmq in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 9)) (22.0.3)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: tornado in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 9)) (6.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: torchfile in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 9)) (0.1.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: websocket-client in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 9)) (0.58.0)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: jsonpatch in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 9)) (1.32)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.6/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 9)) (2.1)\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Building wheels for collected packages: dalle-pytorch, axial-positional-embedding, deepspeed, PyYAML, promise, subprocess32, idna-ssl, ftfy, pathtools\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for dalle-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for dalle-pytorch: filename=dalle_pytorch-0.9.5-py3-none-any.whl size=1375964 sha256=26629582fd6116012e0dc066c2658b34db34deaa2dabe524923c50fc8706cbaf\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-rmph06ms/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2903 sha256=9938d2023961c4bd38c5d7bc88eff21c1cc4a30ae392bbd51f9c90a16af73624\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/87/1e/cd/b5de135ee3faf0c4c525553227e601b0e7739264014dc3e98f\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.3.14-py3-none-any.whl size=337870 sha256=4277a0fc5dc4b2b33b485a290438dc28bf97e60c5357ab0abc8b1b2b0134e6b0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/69/4b/b8/0997fe770ef6af6809bbc20c1e2733660b7a3825452be50248\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=2ea1e6581e5632c01388c9c6d3ac65cdb62b2e9d086ea8bf33f28eb74605e20b\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=48f8987ffb8d99231a42d35dadd4aeed37144f6a110d46a2b72f87199cf94b09\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=8d7a4dc6edf3db8a20304034cfe75731df78b50c6087fb6305aa526fe778cbf0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/44/3a/ab/102386d84fe551b6cedb628ed1e74c5f5be76af8b909aeda09\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=ac0fec91b23dbdde631de524c468c3ce197261531e02b62fa13e0f1076635594\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.1-py3-none-any.whl size=41574 sha256=18f79e2ef6a8a6bcc3604411da0426fae4bc568be6a2679d53ed1454883ab3fc\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/27/9b/c3/6390bcfc12e6250ff2123cd608ccf738479fe2576cf0d24ee8\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=4a6b904f6ece19838f0787be2b6956f1e7d084027bfd3a9fa5b28d83dc91a1f9\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Successfully built dalle-pytorch axial-positional-embedding deepspeed PyYAML promise subprocess32 idna-ssl ftfy pathtools\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Installing collected packages: pyasn1-modules, oauthlib, multidict, cachetools, yarl, requests-oauthlib, idna-ssl, google-auth, async-timeout, tensorboard-plugin-wit, markdown, grpcio, google-auth-oauthlib, aiohttp, absl-py, xmltodict, typed-ast, torchmetrics, toml, tifffile, tensorboard, smmap, PyYAML, PyWavelets, pycryptodomex, py, pluggy, mypy-extensions, iniconfig, imageio, filelock, Shapely, scikit-image, pytorch-lightning, pytest, opencv-python, omegaconf, mypy, gitdb, blobfile, tensorboardX, taming-transformers, subprocess32, shortuuid, sentry-sdk, regex, promise, pathtools, opencv-python-headless, ninja, imgaug, GitPython, ftfy, einops, docker-pycreds, DALL-E, configparser, axial-positional-embedding, wandb, deepspeed, dalle-pytorch, albumentations\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m   Attempting uninstall: PyYAML\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     Found existing installation: PyYAML 5.4.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     Uninstalling PyYAML-5.4.1:\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m       Successfully uninstalled PyYAML-5.4.1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Successfully installed DALL-E-0.1 GitPython-3.1.14 PyWavelets-1.1.1 PyYAML-5.3.1 Shapely-1.7.1 absl-py-0.12.0 aiohttp-3.7.4.post0 albumentations-0.5.2 async-timeout-3.0.1 axial-positional-embedding-0.2.1 blobfile-0.11.0 cachetools-4.2.1 configparser-5.0.2 dalle-pytorch-0.9.5 deepspeed-0.3.14 docker-pycreds-0.4.0 einops-0.3.0 filelock-3.0.12 ftfy-6.0.1 gitdb-4.0.7 google-auth-1.29.0 google-auth-oauthlib-0.4.4 grpcio-1.37.0 idna-ssl-1.1.0 imageio-2.9.0 imgaug-0.4.0 iniconfig-1.1.1 markdown-3.3.4 multidict-5.1.0 mypy-0.812 mypy-extensions-0.4.3 ninja-1.10.0.post2 oauthlib-3.1.0 omegaconf-2.0.6 opencv-python-4.5.1.48 opencv-python-headless-4.5.1.48 pathtools-0.1.2 pluggy-0.13.1 promise-2.3 py-1.10.0 pyasn1-modules-0.2.8 pycryptodomex-3.10.1 pytest-6.2.3 pytorch-lightning-1.2.8 regex-2021.4.4 requests-oauthlib-1.3.0 scikit-image-0.17.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 taming-transformers-0.0.1 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorboardX-1.8 tifffile-2020.9.3 toml-0.10.2 torchmetrics-0.2.0 typed-ast-1.4.3 wandb-0.10.26 xmltodict-0.12.0 yarl-1.6.3\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:40:35,355 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:40:35,355 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:40:35,357 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:40:35,358 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1-1svdy'] Hosts: ['algo-1-1svdy:2'] process_per_hosts: 2 num_processes: 2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:40:35,359 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m 2021-04-17 06:40:35,439 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Training Env:\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"sagemaker_mpi_enabled\": true,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"sagemaker_mpi_num_of_processes_per_host\": 2,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"sagemaker_mpi_custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0 \",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"sagemaker_instance_type\": \"local_gpu\"\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     },\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     },\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"current_host\": \"algo-1-1svdy\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"hosts\": [\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"algo-1-1svdy\"\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     ],\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"EPOCHS\": 1,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"BATCH_SIZE\": 32,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"LEARNING_RATE\": 0.001,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"LR_DECAY_RATE\": 0.98,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"NUM_TOKENS\": 8192,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"NUM_LAYERS\": 2,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"NUM_RESNET_BLOCKS\": 2,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"SMOOTH_L1_LOSS\": false,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"EMB_DIM\": 512,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"HID_DIM\": 256,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"KL_LOSS_WEIGHT\": 0,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"STARTING_TEMP\": 1.0,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"TEMP_MIN\": 0.5,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"ANNEAL_RATE\": 1e-06,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"NUM_IMAGES_SAVE\": 4,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"model_parallel\": true,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"num_microbatches\": 8,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"num-partitions\": 2,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"placement_strategy\": \"spread\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"pipeline\": \"interleaved\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"optimize\": \"speed\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"ddp\": true,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"mp_parameters\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m             \"partitions\": 2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         }\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     },\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"training\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         }\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     },\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"job_name\": \"dalle-poc-exp1-test-1-d-0417-0639\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"master_hostname\": \"algo-1-1svdy\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"module_name\": \"train_vae\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"current_host\": \"algo-1-1svdy\",\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         \"hosts\": [\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m             \"algo-1-1svdy\"\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m         ]\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     },\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m     \"user_entry_point\": \"train_vae.py\"\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m }\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Environment variables:\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HOSTS=[\"algo-1-1svdy\"]\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HPS={\"ANNEAL_RATE\":1e-06,\"BATCH_SIZE\":32,\"EMB_DIM\":512,\"EPOCHS\":1,\"HID_DIM\":256,\"KL_LOSS_WEIGHT\":0,\"LEARNING_RATE\":0.001,\"LR_DECAY_RATE\":0.98,\"NUM_IMAGES_SAVE\":4,\"NUM_LAYERS\":2,\"NUM_RESNET_BLOCKS\":2,\"NUM_TOKENS\":8192,\"SMOOTH_L1_LOSS\":false,\"STARTING_TEMP\":1.0,\"TEMP_MIN\":0.5,\"ddp\":true,\"model_parallel\":true,\"mp_parameters\":{\"partitions\":2},\"num-partitions\":2,\"num_microbatches\":8,\"optimize\":\"speed\",\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_USER_ENTRY_POINT=train_vae.py\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-1svdy\",\"hosts\":[\"algo-1-1svdy\"]}\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_CURRENT_HOST=algo-1-1svdy\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_MODULE_NAME=train_vae\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"local_gpu\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-1svdy\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-1svdy\"],\"hyperparameters\":{\"ANNEAL_RATE\":1e-06,\"BATCH_SIZE\":32,\"EMB_DIM\":512,\"EPOCHS\":1,\"HID_DIM\":256,\"KL_LOSS_WEIGHT\":0,\"LEARNING_RATE\":0.001,\"LR_DECAY_RATE\":0.98,\"NUM_IMAGES_SAVE\":4,\"NUM_LAYERS\":2,\"NUM_RESNET_BLOCKS\":2,\"NUM_TOKENS\":8192,\"SMOOTH_L1_LOSS\":false,\"STARTING_TEMP\":1.0,\"TEMP_MIN\":0.5,\"ddp\":true,\"model_parallel\":true,\"mp_parameters\":{\"partitions\":2},\"num-partitions\":2,\"num_microbatches\":8,\"optimize\":\"speed\",\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"dalle-poc-exp1-test-1-d-0417-0639\",\"log_level\":20,\"master_hostname\":\"algo-1-1svdy\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train_vae\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-1svdy\",\"hosts\":[\"algo-1-1svdy\"]},\"user_entry_point\":\"train_vae.py\"}\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_USER_ARGS=[\"--ANNEAL_RATE\",\"1e-06\",\"--BATCH_SIZE\",\"32\",\"--EMB_DIM\",\"512\",\"--EPOCHS\",\"1\",\"--HID_DIM\",\"256\",\"--KL_LOSS_WEIGHT\",\"0\",\"--LEARNING_RATE\",\"0.001\",\"--LR_DECAY_RATE\",\"0.98\",\"--NUM_IMAGES_SAVE\",\"4\",\"--NUM_LAYERS\",\"2\",\"--NUM_RESNET_BLOCKS\",\"2\",\"--NUM_TOKENS\",\"8192\",\"--SMOOTH_L1_LOSS\",\"False\",\"--STARTING_TEMP\",\"1.0\",\"--TEMP_MIN\",\"0.5\",\"--ddp\",\"True\",\"--model_parallel\",\"True\",\"--mp_parameters\",\"partitions=2\",\"--num-partitions\",\"2\",\"--num_microbatches\",\"8\",\"--optimize\",\"speed\",\"--pipeline\",\"interleaved\",\"--placement_strategy\",\"spread\"]\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_BATCH_SIZE=32\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_LEARNING_RATE=0.001\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_LR_DECAY_RATE=0.98\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_NUM_TOKENS=8192\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_NUM_LAYERS=2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_NUM_RESNET_BLOCKS=2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_SMOOTH_L1_LOSS=false\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_EMB_DIM=512\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_HID_DIM=256\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_KL_LOSS_WEIGHT=0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_STARTING_TEMP=1.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_TEMP_MIN=0.5\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_ANNEAL_RATE=1e-06\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_NUM_IMAGES_SAVE=4\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_MODEL_PARALLEL=true\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_NUM_MICROBATCHES=8\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_NUM-PARTITIONS=2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_PLACEMENT_STRATEGY=spread\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_PIPELINE=interleaved\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_OPTIMIZE=speed\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_DDP=true\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m SM_HP_MP_PARAMETERS={\"partitions\":2}\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m mpirun --host algo-1-1svdy:2 -np 2 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_EPOCHS -x SM_HP_BATCH_SIZE -x SM_HP_LEARNING_RATE -x SM_HP_LR_DECAY_RATE -x SM_HP_NUM_TOKENS -x SM_HP_NUM_LAYERS -x SM_HP_NUM_RESNET_BLOCKS -x SM_HP_SMOOTH_L1_LOSS -x SM_HP_EMB_DIM -x SM_HP_HID_DIM -x SM_HP_KL_LOSS_WEIGHT -x SM_HP_STARTING_TEMP -x SM_HP_TEMP_MIN -x SM_HP_ANNEAL_RATE -x SM_HP_NUM_IMAGES_SAVE -x SM_HP_MODEL_PARALLEL -x SM_HP_NUM_MICROBATCHES -x SM_HP_NUM-PARTITIONS -x SM_HP_PLACEMENT_STRATEGY -x SM_HP_PIPELINE -x SM_HP_OPTIMIZE -x SM_HP_DDP -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py -m train_vae --ANNEAL_RATE 1e-06 --BATCH_SIZE 32 --EMB_DIM 512 --EPOCHS 1 --HID_DIM 256 --KL_LOSS_WEIGHT 0 --LEARNING_RATE 0.001 --LR_DECAY_RATE 0.98 --NUM_IMAGES_SAVE 4 --NUM_LAYERS 2 --NUM_RESNET_BLOCKS 2 --NUM_TOKENS 8192 --SMOOTH_L1_LOSS False --STARTING_TEMP 1.0 --TEMP_MIN 0.5 --ddp True --model_parallel True --mp_parameters partitions=2 --num-partitions 2 --num_microbatches 8 --optimize speed --pipeline interleaved --placement_strategy spread\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m  Data for JOB [50364,1] offset 0 Total slots allocated 2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m  ========================   JOB MAP   ========================\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m  Data for node: algo-1-1svdy\tNum slots: 2\tMax slots: 0\tNum procs: 2\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m  \tProcess OMPI jobid: [50364,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m  \tProcess OMPI jobid: [50364,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m \n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m  =============================================================\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:256 [0] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:256 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:256 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:256 [0] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:256 [0] NCCL INFO Using network Socket\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:NCCL version 2.7.8+cuda11.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:257 [1] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:257 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:257 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:257 [1] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:257 [1] NCCL INFO Using network Socket\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 00/04 :    0   1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 01/04 :    0   1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 02/04 :    0   1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 03/04 :    0   1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] -1/-1/-1->1->0|0->1->-1/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] -1/-1/-1->1->0|0->1->-1/-1/-1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->-1|-1->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->-1|-1->0->1/-1/-1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO Channel 00 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 00 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO Channel 01 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 01 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO Channel 02 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 02 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO Channel 03 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO Channel 03 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:453 [1] NCCL INFO comm 0x7fec8c001060 rank 1 nranks 2 cudaDev 1 busId 180 - Init COMPLETE\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:452 [0] NCCL INFO comm 0x7f274c001060 rank 0 nranks 2 cudaDev 0 busId 170 - Init COMPLETE\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:256 [0] NCCL INFO Launch mode Parallel\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:[2021-04-17 06:40:41.306: I smdistributed/modelparallel/torch/state_mod.py:108] [1] Finished initializing torch distributed process groups. pp_rank: 1, dp_rank: 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:41.306: I smdistributed/modelparallel/torch/state_mod.py:108] [0] Finished initializing torch distributed process groups. pp_rank: 0, dp_rank: 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:args.image_folder : ../../dataset/val2017\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:args.image_folder : ../../dataset/val2017\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:args.rank : 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:args.rank : 1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:IMAGE_PATH : /opt/ml/input/data/training\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:IMAGE_PATH : /opt/ml/input/data/training\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:41.526 algo-1-1svdy:256 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:[2021-04-17 06:40:41.526 algo-1-1svdy:257 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:41.556 algo-1-1svdy:256 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:[2021-04-17 06:40:41.557 algo-1-1svdy:257 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:41.994: I smdistributed/modelparallel/torch/worker.py:281] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.901: I smdistributed/modelparallel/torch/model.py:231] Partition assignments:\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.901: I smdistributed/modelparallel/torch/model.py:238] main: 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.902: I smdistributed/modelparallel/torch/model.py:238] main/module: 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.902: I smdistributed/modelparallel/torch/model.py:238] main/module/module: 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.902: I smdistributed/modelparallel/torch/model.py:238] main/module/module/codebook: 1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.902: I smdistributed/modelparallel/torch/model.py:238] main/module/module/encoder: 1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.902: I smdistributed/modelparallel/torch/model.py:238] main/module/module/decoder: 0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.902: I smdistributed/modelparallel/torch/model.py:238] main/module/module/einsum: 1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:[2021-04-17 06:40:43.936: I smdistributed/modelparallel/torch/model.py:181] Number of parameters on partition 1 are 19. 19 require grads\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.957: I smdistributed/modelparallel/torch/model.py:181] Number of parameters on partition 0 are 20. 20 require grads\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:43.958: I smdistributed/modelparallel/torch/model.py:260] Finished partitioning the model\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:NCCL version 2.7.8+cuda11.0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 00/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 01/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 02/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 03/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 04/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 05/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 06/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 07/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 08/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 09/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 10/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 11/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 12/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 13/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 14/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 15/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 16/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 17/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 18/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 19/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 20/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 21/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 22/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 23/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 24/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 25/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 26/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 27/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 28/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 00/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 01/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 29/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 30/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 02/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Channel 31/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 03/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 04/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 05/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 06/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 07/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 08/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 09/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 10/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 11/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 12/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 13/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 14/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 15/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 16/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 17/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 18/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 19/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 20/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 21/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 22/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 23/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 24/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 25/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 26/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 27/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 28/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 29/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 30/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Channel 31/32 :    0\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->-1\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:algo-1-1svdy:256:504 [0] NCCL INFO comm 0x7f2654001060 rank 0 nranks 1 cudaDev 0 busId 170 - Init COMPLETE\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,1]<stdout>:algo-1-1svdy:257:503 [1] NCCL INFO comm 0x7fec40001060 rank 0 nranks 1 cudaDev 1 busId 180 - Init COMPLETE\n",
      "\u001b[36m5y6zfj3v6k-algo-1-1svdy |\u001b[0m [1,0]<stdout>:[2021-04-17 06:40:46.570: I smdistributed/modelparallel/torch/model.py:307] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\n"
     ]
    }
   ],
   "source": [
    "create_experiment(experiment_name)\n",
    "job_name = create_trial(experiment_name, hyperparameters, instance_type, instance_count, do_spot_training)\n",
    "\n",
    "# Now associate the estimator with the Experiment and Trial\n",
    "estimator.fit(\n",
    "    inputs={'training': s3_data_path}, \n",
    "    job_name=job_name,\n",
    "    experiment_config={\n",
    "      'TrialName': job_name,\n",
    "      'TrialComponentDisplayName': job_name,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name=estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Aynchronous</strong>로 진행된 Training job은 아래와 같은 방법으로 진행상황을 실시간으로 확인할 수 있습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {rule_output_path}/ProfilerReport/profiler-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {rule_output_path}/ProfilerReport/profiler-output/ {output_dir}/ProfilerReport/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>ProfilerReport : <a href=\"{}profiler-report.html\">Profiler Report</a></b>'.format(output_dir+\"/ProfilerReport/\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store hyperparameters model_dir output_dir artifacts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<p>Amazon SageMaker에서 모든 학습을 완료하였습니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_sync.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sync.py\n",
    "\n",
    "def aws_s3_sync(source, destination):\n",
    "    \n",
    "    \"\"\"aws s3 sync in quiet mode and time profile\"\"\"\n",
    "    import time, subprocess\n",
    "    cmd = [\"aws\", \"s3\", \"sync\", \"--quiet\", source, destination]\n",
    "    print(f\"Syncing files from {source} to {destination}\")\n",
    "    start_time = time.time()\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    p.wait()\n",
    "    end_time = time.time()\n",
    "    print(\"Time Taken to Sync: \", (end_time-start_time))\n",
    "    return\n",
    "\n",
    "\n",
    "def sync_local_checkpoints_to_s3(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\n",
    "    \n",
    "    \"\"\" sample function to sync checkpoints from local path to s3 \"\"\"\n",
    "\n",
    "    import boto3, botocore\n",
    "    #check if local path exists\n",
    "    if not os.path.exists(local_path):\n",
    "        raise RuntimeError(\"Provided local path {local_path} does not exist. Please check\")\n",
    "\n",
    "    #check if s3 bucket exists\n",
    "    s3 = boto3.resource('s3')\n",
    "    if 's3://' not in s3_path:\n",
    "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\n",
    "\n",
    "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\n",
    "    print(f\"S3 Bucket: {s3_bucket}\")\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=s3_bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            raise RuntimeError('S3 bucket does not exist. Please check')\n",
    "    aws_s3_sync(local_path, s3_path)\n",
    "    return\n",
    "\n",
    "def sync_s3_checkpoints_to_local(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\n",
    "    \n",
    "    \"\"\" sample function to sync checkpoints from s3 to local path \"\"\"\n",
    "\n",
    "    import boto3, botocore\n",
    "    #creat if local path does not exists\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Provided local path {local_path} does not exist. Creating...\")\n",
    "        try:\n",
    "            os.makedirs(local_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"failed to create {local_path}\")\n",
    "\n",
    "    #check if s3 bucket exists\n",
    "    s3 = boto3.resource('s3')\n",
    "    if 's3://' not in s3_path:\n",
    "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\n",
    "\n",
    "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\n",
    "    print(f\"S3 Bucket: {s3_bucket}\")\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=s3_bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            raise RuntimeError('S3 bucket does not exist. Please check')\n",
    "    aws_s3_sync(s3_path, local_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_local_checkpoints_to_s3(local_path='/opt/ml/local_checkpoints', s3_path=full_s3_path)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
