{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Multigpu Distributed Training-ScriptMode\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API을 효과적으로 이용하기 위해 multigpu-distributed 학습을 위한 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "#     !{sys.executable} -m pip install -U split-folders tqdm albumentations crc32c wget\n",
    "    !{sys.executable} -m pip install 'sagemaker[local]' --upgrade\n",
    "    !{sys.executable} -m pip install -U bokeh smdebug sagemaker-experiments\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    !/bin/bash ./local/local_mode_setup.sh\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정\n",
    "\n",
    "<p>Sagemaker 학습에 필요한 기본적인 package를 import 합니다. </p>\n",
    "<p>boto3는 HTTP API 호출을 숨기는 편한 추상화 모델을 가지고 있고, Amazon EC2 인스턴스 및 S3 버켓과 같은 AWS 리소스와 동작하는 파이선 클래스를 제공합니다. </p>\n",
    "<p>sagemaker python sdk는 Amazon SageMaker에서 기계 학습 모델을 교육 및 배포하기 위한 오픈 소스 라이브러리입니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "# import splitfolders\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "# import wget\n",
    "# import tarfile\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from time import strftime\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "from sagemaker.debugger import (Rule,\n",
    "                                rule_configs,\n",
    "                                ProfilerConfig, \n",
    "                                FrameworkProfile, \n",
    "                                DetailedProfilingConfig, \n",
    "                                DataloaderProfilingConfig, \n",
    "                                PythonProfilingConfig)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name):\n",
    "    try:\n",
    "        sm_experiment = Experiment.load(experiment_name)\n",
    "    except:\n",
    "        sm_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                          tags=[\n",
    "                                              {\n",
    "                                                  'Key': 'multigpu',\n",
    "                                                  'Value': 'yes'\n",
    "                                              },\n",
    "                                              {\n",
    "                                                  'Key': 'multinode',\n",
    "                                                  'Value': 'yes'\n",
    "                                              },\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial(experiment_name, set_param, i_type, i_cnt, spot):\n",
    "    create_date = strftime(\"%m%d-%H%M%s\")\n",
    "    \n",
    "    spot = 's' if spot else 'd'\n",
    "    i_tag = 'test'\n",
    "    if i_type == 'ml.p3.16xlarge':\n",
    "        i_tag = 'p3'\n",
    "    elif i_type == 'ml.p3dn.24xlarge':\n",
    "        i_tag = 'p3dn'\n",
    "    elif i_type == 'ml.p4d.24xlarge':\n",
    "        i_tag = 'p4d'    \n",
    "        \n",
    "    trial = \"-\".join([i_tag,str(i_cnt),spot])\n",
    "       \n",
    "    sm_trial = Trial.create(trial_name=f'{experiment_name}-{trial}-{create_date}',\n",
    "                            experiment_name=experiment_name)\n",
    "\n",
    "    job_name = f'{sm_trial.trial_name}'\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'bucket-exp-dalle-210410'\n",
    "code_location = f's3://{bucket}/sm_codes'\n",
    "output_path = f's3://{bucket}/poc_dalle/output/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'train:lr', 'Regex': 'lr - (.*?),'},\n",
    "     {'Name': 'train:Loss', 'Regex': 'loss -(.*?),'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "\n",
    "rules=[ \n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'EPOCHS' : 50,\n",
    "        'BATCH_SIZE' : 48, # 24\n",
    "        'LEARNING_RATE' : 1e-3, # 1e-3\n",
    "        'LR_DECAY_RATE' : 0.98,\n",
    "        'NUM_TOKENS' : 8192,\n",
    "        'NUM_LAYERS' : 2,\n",
    "        'NUM_RESNET_BLOCKS' : 2,\n",
    "        'SMOOTH_L1_LOSS' : False,\n",
    "        'EMB_DIM' : 512,\n",
    "        'HID_DIM' : 256,\n",
    "        'KL_LOSS_WEIGHT' : 0,\n",
    "        'STARTING_TEMP' : 1.,\n",
    "        'TEMP_MIN' : 0.5,\n",
    "        'ANNEAL_RATE' : 1e-6,\n",
    "        'NUM_IMAGES_SAVE' : 4,\n",
    "        'model_parallel': True,  ## False : DeepSpeeds\n",
    "        'num_microbatches': 8,\n",
    "        'num-partitions' : 4,\n",
    "        'placement_strategy': 'spread',\n",
    "        'pipeline': 'interleaved',\n",
    "        'optimize': 'speed',\n",
    "        'ddp': True,\n",
    "    }\n",
    "\n",
    "experiment_name = 'dalle-poc-exp1'\n",
    "instance_type = 'ml.p4d.24xlarge'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "instance_count = 2\n",
    "do_spot_training = False\n",
    "max_wait = None\n",
    "max_run = 12*60*60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "    from pathlib import Path\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    s3_data_path = 'file:///home/ec2-user/SageMaker/napkin-Dalle/dataset'\n",
    "    source_dir = f'{Path.cwd()}/source_code'\n",
    "else:\n",
    "    sess = boto3.Session()\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sm = sess.client('sagemaker')\n",
    "    bucket_name = 'dataset-cyj-coco-210410'\n",
    "    s3_data_path = f's3://{bucket_name}/dataset'\n",
    "    source_dir = 'source_code'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = None\n",
    "distribution = None\n",
    "train_job_name = 'sagemaker'\n",
    "\n",
    "\n",
    "train_job_name = 'smp-dist'\n",
    "\n",
    "distribution = {\"smdistributed\": {\n",
    "                  \"modelparallel\": {\n",
    "                      \"enabled\":True,\n",
    "                      \"parameters\": {\n",
    "                          \"partitions\": hyperparameters['num-partitions'],\n",
    "#                               \"microbatches\": 8,\n",
    "#                               \"placement_strategy\": \"spread\",\n",
    "#                               \"pipeline\": \"interleaved\",\n",
    "#                               \"optimize\": \"speed\",\n",
    "#                               \"partitions\": 1,\n",
    "#                               \"ddp\": True,\n",
    "                      }\n",
    "                  }\n",
    "              },\n",
    "              \"mpi\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"processes_per_host\": 4, # Pick your processes_per_host\n",
    "                    \"custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0 \"\n",
    "              },\n",
    "          }\n",
    "\n",
    "\n",
    "if do_spot_training:\n",
    "    max_wait = max_run\n",
    "\n",
    "print(\"train_job_name : {} \\ntrain_instance_type : {} \\ntrain_instance_count : {} \\nimage_uri : {} \\ndistribution : {}\".format(train_job_name, instance_type, instance_count, image_uri, distribution))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='train_vae.py',\n",
    "    source_dir=source_dir,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "#     volume_size=1024,\n",
    "    code_location = code_location,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution=distribution,\n",
    "#     disable_profiler=True,\n",
    "    metric_definitions=metric_definitions,\n",
    "    rules=rules,\n",
    "    max_run=max_run,\n",
    "    use_spot_instances=do_spot_training,  # spot instance 활용\n",
    "    max_wait=max_wait,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_experiment(experiment_name)\n",
    "job_name = create_trial(experiment_name, hyperparameters, instance_type, instance_count, do_spot_training)\n",
    "\n",
    "# Now associate the estimator with the Experiment and Trial\n",
    "estimator.fit(\n",
    "    inputs={'training': s3_data_path}, \n",
    "    job_name=job_name,\n",
    "    experiment_config={\n",
    "      'TrialName': job_name,\n",
    "      'TrialComponentDisplayName': job_name,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name=estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Aynchronous</strong>로 진행된 Training job은 아래와 같은 방법으로 진행상황을 실시간으로 확인할 수 있습니다.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {rule_output_path}/ProfilerReport/profiler-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {rule_output_path}/ProfilerReport/profiler-output/ {output_dir}/ProfilerReport/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>ProfilerReport : <a href=\"{}profiler-report.html\">Profiler Report</a></b>'.format(output_dir+\"/ProfilerReport/\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store hyperparameters model_dir output_dir artifacts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<p>Amazon SageMaker에서 모든 학습을 완료하였습니다. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_sync.py\n",
    "\n",
    "def aws_s3_sync(source, destination):\n",
    "    \n",
    "    \"\"\"aws s3 sync in quiet mode and time profile\"\"\"\n",
    "    import time, subprocess\n",
    "    cmd = [\"aws\", \"s3\", \"sync\", \"--quiet\", source, destination]\n",
    "    print(f\"Syncing files from {source} to {destination}\")\n",
    "    start_time = time.time()\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    p.wait()\n",
    "    end_time = time.time()\n",
    "    print(\"Time Taken to Sync: \", (end_time-start_time))\n",
    "    return\n",
    "\n",
    "\n",
    "def sync_local_checkpoints_to_s3(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\n",
    "    \n",
    "    \"\"\" sample function to sync checkpoints from local path to s3 \"\"\"\n",
    "\n",
    "    import boto3, botocore\n",
    "    #check if local path exists\n",
    "    if not os.path.exists(local_path):\n",
    "        raise RuntimeError(\"Provided local path {local_path} does not exist. Please check\")\n",
    "\n",
    "    #check if s3 bucket exists\n",
    "    s3 = boto3.resource('s3')\n",
    "    if 's3://' not in s3_path:\n",
    "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\n",
    "\n",
    "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\n",
    "    print(f\"S3 Bucket: {s3_bucket}\")\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=s3_bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            raise RuntimeError('S3 bucket does not exist. Please check')\n",
    "    aws_s3_sync(local_path, s3_path)\n",
    "    return\n",
    "\n",
    "def sync_s3_checkpoints_to_local(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\n",
    "    \n",
    "    \"\"\" sample function to sync checkpoints from s3 to local path \"\"\"\n",
    "\n",
    "    import boto3, botocore\n",
    "    #creat if local path does not exists\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Provided local path {local_path} does not exist. Creating...\")\n",
    "        try:\n",
    "            os.makedirs(local_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"failed to create {local_path}\")\n",
    "\n",
    "    #check if s3 bucket exists\n",
    "    s3 = boto3.resource('s3')\n",
    "    if 's3://' not in s3_path:\n",
    "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\n",
    "\n",
    "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\n",
    "    print(f\"S3 Bucket: {s3_bucket}\")\n",
    "    try:\n",
    "        s3.meta.client.head_bucket(Bucket=s3_bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            raise RuntimeError('S3 bucket does not exist. Please check')\n",
    "    aws_s3_sync(s3_path, local_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_local_checkpoints_to_s3(local_path='/opt/ml/local_checkpoints', s3_path=full_s3_path)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
